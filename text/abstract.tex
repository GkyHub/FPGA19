\begin{abstract}

Training convolutional neural network (CNN) usually requires large amount of computation resources, time and power. Researchers and cloud service providers in this region needs fast and efficient training system. GPU is currently the best candidate for DNN training. But FPGAs have already shown good performance and energy efficiency as CNN inference accelerators. In this work, we design an FPGA-based fast and energy efficient CNN training accelerator. We adopt two of the widely used model compression methods, quantization and pruning, to accelerate CNN training process. 

Although quantization and pruning are proved to be efficient for CNN inference, we are still faced with challenges when adopting them in training. 

But accelerating CNN training is still hard. On one hand, current work focuses on training an efficient network rather than efficiently training a network. The training phase is still not hardware friendly enough. On the other hand, the difference between inference phase and back propagation phase in training brings more challenges to hardware design. Existing designs are hard to support training efficiently and utilize sparse property in training.

In this paper, we propose an efficient CNN training method with both software optimization and hardware architecture design. A hardware friendly network training method is proposed with all-fixed-point-data computation and sparse network parameters. An FPGA based architecture is designed to accelerate this training process. Proposed hardware achieves 641GOP/s equivalent performance and 3x better energy efficiency compared with GPU. 

\end{abstract}