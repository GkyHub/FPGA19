\section{Related Work}\label{sec:rw}
In this section, we introduce related work on neural network training and hardware accelerator design respectively. For training, we focus on data quantization in training and network pruning. For accelerator, we introduce both inference and training accelerators.

\subsection{Training with Fixed Point Data}\label{sec:rw:fixed_train}
We usually adopt 32-bit floating point data in both inference and training of neural networks. An effective way for neural network inference acceleration is to use fewer bits for data in NN models to reduce both memory and computation cost. Previous researches~\cite{han2016eie,qiu2016going} show that 8 or even fewer bits can be used to achieve significant acceleration on customized hardware architectures with negligible accuracy loss. 

Other studies have tried to use fixed-point gradients~\cite{hubara2016binarized, hubara2017quantized, zhou2016dorefa} in neural network training. The weights, activations, and gradients are all quantized in these work. Although the bit-widths of data blobs are low in these work, they all require storing floating-point scale factors with floating-point operations for multiplication and gradient distribution statistics.  {\bf{Floating point operations are still of high cost while total fixed-point based training process still needs research.}} For example, Zhou et al.~\cite{zhou2016dorefa} quantize the weights to 1 bit, activations to 2 bit, and the gradients of activations to 6 bit. The quantization method is to uniformly quantize each blob between $[-\mbox{absmax}, \mbox{absmax}]$, and a floating-point $\mbox{absmax}$ value must be stored for each blob. So the quantization process introduces more floating point operations.

\subsection{Neural Network Pruning}\label{sec:rw:prune}
Pruning is an efficient method to reduce both the model size and the number of operations by removing insignificant parameters, which helps achieve neural network acceleration. Han et al.~\cite{han2015learning} introduce a direct method by iteratively pruning parameters below a threshold and fine-tuning. This method reduces network parameters to $1/9-1/13$. However, the pruning is done with each single parameter, referred to as 0-dimension pruning. The irregularity of the pruned model brings a challenge for executing the network efficiently on hardware. Some researches focus on exploring group-wise sparsity (1-dimension or 2-dimension pruning)\cite{lebedev2016fast,zhou2016less,wen2016learning} of the convolution filters. Recently, filter-wise pruning (3-dimension pruning)\cite{molchanov2016pruning,li2016pruning} is proposed to reduce model size while totally keeping the model regularity. As suggested by Mao et al.\cite{mao2017exploring}, 0 to 2-dimension pruning does negligible harm to the network accuracy while 3-dimension pruning incurs great accuracy loss. In this work, we choose 2-dimension pruning which keeps acceptable model accuracy and relatively good regularity.

\subsection{CNN Inference Accelerator}\label{sec:rw:inference_acc}
Many hardware designs have been proposed to accelerate dense CNN inference. As suggested by~\cite{ma2017optimizing}, for a single layer, a CNN inference accelerator design involves three aspects: loop unrolling, loop tiling, and loop interchange. Loop unrolling strategy decides the parallelism or the peak performance of the hardware. \cite{zhang2015optimizing, du2015shidiannao, qiu2016going} explore different unroll dimensions and hardware designs respectively. Loop tiling and loop interchange strategy decide how the computation of this layer is scheduled. When the data needs to be loaded from external memory, a good loop tiling and loop interchange strategy helps minimize bandwidth requirement and maximize the utilization of hardware computation power. For CNN training, the back propagation also consists of nested loops. Thus the same design methods can be applied.

For CNN training, as suggested by section~\ref{sec:preliminary}, the loops in back propagation phase is similar to that in inference phase. But the loop dimension varies greatly between the FP and BP phases. For example, if a convolutional layer does $3\times 3$ convolution on $224\times 224$ feature maps with 1 padding, computing the gradient of these convolution kernels needs $224\times 224$ convolutions on $224\times 224$ feature maps with 1 padding. {\bf{This causes a great variety in convolution kernels and convolution result sizes and thus limits the loop unrolling design.}} 

With the research progress in neural network model pruning, more and more work focus on accelerating sparse CNN inference. One kind of work focus on utilizing the sparsity of network parameters~\cite{han2017ese,zhang2016cambricon}. Albericio, et al.\cite{albericio2016cnvlutin} utilize the data sparsity brought by ReLU function. Han et al. \cite{han2016eie} utilizes both the sparsity of data and parameters. We refer to these kind of designs as operator-sparse type, which means the one or both the operators of MAC functions are sparse. For CNN training with sparse parameters, a new type of sparse computation, result-sparse is needed. When calculating the gradient of network parameters, the operators are activations of one layer and the gradients of the activations of the next layer, which can both be dense. But we do not need to compute the gradients of the pruned parameters. So the results are sparse.

\subsection{CNN Training Accelerator}\label{sec:rw:train_acc}
Compared with inference, less studies focus on training acceleration. Previous accelerator designs~\cite{liu2017fpga, zhao2016f} use 32-bit floating point for computation. Both the performance and energy efficiency of the accelerator are not competitive compared with GPU or state-of-the-art inference accelerators. Geng et al. propose the FPDeep~\cite{geng2018fpdeep} framework to map the training process on multiple FPGAs, which offers a high performance solution. Compared with previous work, FPDeep uses 16-bit fixed point data for training and designs independent modules for FP, EB and WG phases respectively. But the hardware still does not utilize sparsity in training. In this paper, we propose a hardware architecture that utilize both operator-sparsity and result-sparsity.

