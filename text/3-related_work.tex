\section{Related Work}\label{sec:rw}
In this section, we introduce the related work on neural network training and hardware accelerator design respectively. For training, we focus on data quantization and network pruning. For hardware accelerator, we introduce both inference and training accelerators.

\subsection{Training with Fixed Point Data}\label{sec:rw:fixed_train}
We usually adopt 32-bit floating point data in both inference and training of neural networks. An effective way for neural network inference acceleration is to use fewer bits for data in NN models to reduce both memory and computation cost. Previous researches~\cite{han2016eie,qiu2016going} show that 8 or even fewer bits can be used to achieve significant acceleration on customized hardware architectures with negligible accuracy loss. 

Other studies have tried to use fixed-point gradients~\cite{hubara2016binarized, hubara2017quantized, zhou2016dorefa} in neural network training. Although these work adopts extremely narrow bit-width for weights, activations and gradients, they all require storing floating-point scale factors with floating-point operations for multiplication and gradient distribution statistics. Floating point operations are still of high cost. For example, Zhou et al.~\cite{zhou2016dorefa} quantize the weights to 1 bit, activations to 2 bit, and the gradients of activations to 6 bit. The quantization method is to uniformly quantize each blob between $[-\mbox{absmax}, \mbox{absmax}]$, and a floating-point $\mbox{absmax}$ value must be stored for each blob. So the quantization process introduces more floating point operations.

\subsection{Neural Network Pruning}\label{sec:rw:prune}
Pruning is an efficient method to reduce both the model size and the number of operations by removing insignificant parameters and fine-tune the model, which helps achieve neural network acceleration. Han et al.~\cite{han2015learning} introduce a direct method by iteratively pruning parameters below a threshold. This method reduces network parameters to $1/9-1/13$. However, the pruning is done with each single parameter, referred to as 0-dimension pruning. The irregularity of the pruned model brings a challenge for executing the network efficiently on hardware. Higher dimensional pruning helps regularize the shape of the network and benefits hardware design. Some researches focus on exploring group-wise sparsity (1-dimension or 2-dimension pruning)\cite{lebedev2016fast,zhou2016less,wen2016learning} of the convolution filters. Recently, filter-wise pruning (3-dimension pruning)\cite{molchanov2016pruning,li2016pruning} is proposed to reduce model size while totally keeping the model regularity. As suggested by Mao et al.\cite{mao2017exploring}, 0 to 2-dimension pruning does negligible harm to the network accuracy while 3-dimension pruning incurs great accuracy loss. All the researches above prunes the network after the training process converges. Thus the benefit to accelerate the fine-tune process after pruning is limited.

\subsection{CNN Inference Accelerator}\label{sec:rw:inference_acc}
Many hardware designs have been proposed to accelerate dense CNN inference. As suggested by~\cite{ma2017optimizing}, for a single layer, a CNN inference accelerator design involves three aspects: loop unrolling, loop tiling, and loop interchange. Loop unrolling strategy decides the parallelism or the peak performance of the hardware. \cite{zhang2015optimizing, du2015shidiannao, qiu2016going} explore different unroll dimensions and hardware designs respectively. Loop tiling and interchange strategy decides how the computation of a layer is scheduled. When the data needs to be loaded from external memory, a good loop tiling and interchange strategy helps minimize bandwidth requirement and maximize the utilization of hardware computation power. For CNN training, the back propagation also consists of nested loops. Thus the same design methods can be applied.

With the research progress in neural network model pruning, more and more work focus on accelerating sparse CNN inference. One kind of work focus on utilizing the sparsity of network parameters~\cite{han2017ese,zhang2016cambricon}. Albericio, et al.\cite{albericio2016cnvlutin} utilize the data sparsity brought by ReLU function. Han et al. \cite{han2016eie} utilizes both the sparsity of data and parameters. We refer to these kind of designs as operator-sparse type, which means the one or both the operators of MAC functions are sparse. For CNN training with sparse parameters, a new type of sparse computation, result-sparse is needed. When calculating the gradient of network parameters, the operators are activations of one layer and the gradients of the activations of the next layer, which can both be dense. But we do not need to compute the gradients of the pruned parameters. So the results are sparse.

\subsection{CNN Training Accelerator}\label{sec:rw:train_acc}
Compared with inference, less studies focus on training acceleration. Previous accelerator designs~\cite{liu2017fpga, zhao2016f} use 32-bit floating point for computation. Both the performance and energy efficiency of the accelerator are not competitive compared with GPU or state-of-the-art inference accelerators. Geng et al. propose the FPDeep~\cite{geng2018fpdeep} framework to map the training process on multiple FPGAs, which offers a high performance solution. Compared with previous work, FPDeep uses 16-bit fixed point data for training and designs independent modules for FP, EB and WG phases respectively. But the hardware still does not utilize sparsity in training. In this paper, we propose a hardware architecture that utilize both operator-sparsity and result-sparsity in CNN training.

