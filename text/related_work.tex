\section{Related Work}\label{sec:related_work}
In this section, we will introduce related work on software and hardware respectively. For software, we will focus on neural network training with fixed-point data and pruned parameters. For hardware, as few previous work targets at training acceleration, we will focus on inference accelerators.

\subsection{Training with Fixed Point Data}
Usually, neural network models are trained and used with 32-bit floating point data on GPU or CPU. An effective way for neural network acceleration is to use fewer bits for data in NN models to reduce memory cost and simplify each of the operation.

For inference, previous research~\cite{han2016eie,qiu2016going} show that 8-bit or even fewer bits can be used to achieve significant acceleration on customized hardware architectures with negligible accuracy loss. Courbariaux et al.\cite{hubara2016binarized} demonstrate how to train a binary NN model which brings relatively small accuracy loss on the CIFAR-10 dataset. Tang et al.~\cite{tang2017train} focus on how to enable better training of a BNN(binarized NN) on larger datasets using strategies such as small learning rate, bipolar regularization, etc.

Some work also studies how to perform the training process with fixed-point gradients~\cite{hubara2016binarized, hubara2017quantized, zhou2016dorefa}, in these work, not only the weights and the activations, the gradients in the training process are also quantized. Zhou et al.~\cite{zhou2016dorefa} quantize the weights to 1 bit, activations to 2 bit, and the gradients of activations to 6 bit. Their quantization method is to uniformly quantize each blob between $[-\mbox{absmax}, \mbox{absmax}]$, and a floating-point $\mbox{absmax}$ value must be stored for each blob. To do quantization, they need global information of each blob (calculate the floating-point number of each blob). Although the bit-width of data blobs is low in these work, they all require storing floating-point scale factors with floating-point operations for multiplication and gradient distribution statistics. {\bf{Floating point operations are still of high cost while total fixed-point based training process still needs research.}}


\subsection{Neural Network Pruning}
Pruning is an efficient method to reduce both the model size and the number of operations by removing insignificant parameters, which helps achieve neural network acceleration. Han et al.~\cite{han2015learning} introduced a direct method by iteratively pruning parameters below the threshold and fine-tuning. Such element-wise pruning (0-dimension pruning) is powerful for compression and reduces storage 9x-13x. However, the irregularity of the pruned model brings a challenge for executing the network efficiently on hardware. Some researches focus on exploring group-wise sparsity (1-dimension or 2-dimension pruning)\cite{lebedev2016fast,zhou2016less,wen2016learning} of the convolution filters. Recently, pruning filters (3-dimension pruning)\cite{molchanov2016pruning,li2016pruning} is proposed to reduce model size while totally keeping the model regularity. As suggested by Mao et al.\cite{mao2017exploring}, 0 to 2-dimension pruning does negligible harm to the network accuracy while 3-dimension pruning incurs great accuracy loss. In this work, we choose 2-dimension pruning which keeps acceptable model accuracy and relatively good regularity.

\subsection{Dense CNN Inference Accelerator}\label{sec:rw_dense}
Many hardware designs have been proposed to accelerate dense CNN inference. As suggested by~\cite{ma2017optimizing}, for a single layer, a CNN inference accelerator design involves three aspects: loop unrolling, loop tiling, and loop interchange. Loop unrolling strategy decides the parallelism or the peak performance of the hardware. \cite{zhang2015optimizing, du2015shidiannao, qiu2016going} explore different unroll dimensions and hardware designs respectively. Loop tiling and loop interchange strategy decide how the computation of this layer is scheduled. When the data needs to be loaded from external memory, a good loop tiling and loop interchange strategy helps minimize bandwidth requirement and maximize the utilization of hardware computation power. For CNN training, the back propagation also consists of nested loops. Thus the same design methods can be applied.

%For a whole network, a strategy for one layer is usually hard to be fit into the other layers. Loop tiling and loop interchange can be controlled by software while loop unrolling strategy is hard to change at run-time. One solution is to implement different hardware for different layers as a pipeline~\cite{li2016high}. This method is usually resource consuming and lacks flexibility for different networks. Another way is to use a flexible structure for computation unit array. \cite{chen2016eyeriss} proposes a run-time configurable PE array structure. In this design, all the PEs are configured with certain id and multiple buses are used to adapt to different data transfer behavior under different configurations. However, this kind of bus based structure cannot be efficiently implemented on FPGA.

For CNN training, as suggested by section~\ref{sec:preliminary}, the loops in back propagation phase is similar to that in inference phase. But the loop dimension varies greatly between the feed forward and back propagation phases. For example, if a convolutional layer does $3\times 3$ convolution on $224\times 224$ feature maps with 1 padding, computing the gradient of these convolution kernels needs $224\times 224$ convolutions on $224\times 224$ feature maps with 1 padding. {\bf{This causes great variety in convolution kernels and convolution result sizes and thus limits the loop unroll design.}} 

\subsection{Sparse CNN Inference Accelerator}\label{sec:rw_sparse}
With the research progress in neural network model pruning, more and more work focus on accelerating sparse CNN inference. One kind of work focus on utilizing the sparsity of network parameters~\cite{han2017ese,zhang2016cambricon}, which is denoted by $S\times D=D$ type. \cite{albericio2016cnvlutin} utilizes the data sparsity brought by ReLU function, which is denoted by $D\times S=D$. \cite{han2016eie} utilizes both the sparsity of data and parameters. For CNN training with sparse parameters, a new type of sparse computation, $D\times D=S$ is needed. When calculating the gradient of network parameters, the operators are activation of one layer and the gradient of the activation of the next layer, which can both be dense. But we do not need to compute the gradient of the pruned parameters. So the result is sparse. In this work, we propose an architecture to support both $S\times D=D$ and $D\times D=S$ computation.

