\section{Introduction}\label{sec:introduction}

Convolutional neural networks (CNN) have made significant performance improvement in computer vision tasks \cite{He2016Deep, Shelhamer2017Fully}. However, the accuracy improvement comes at significant cost of both inference and training computation. A wide range of work have been proposed~\cite{qiu2016going, han2017ese} to achieve fast and energy efficient inference, showing outstanding performance over GPU. Few work focuses on the hardware accelerator design for training phase. 

Training neural networks requires large amount of computation resources and can take days or weeks. Fast and energy efficient training is important for researchers and cloud service providers. GPU is currently the most suitable platform for CNN training. Recently, a number of cloud service providers including Microsoft, Amazon, Alibaba, and Huawei have deployed large FPGA clusters in their data centers. This provides the environment for FPGA-based neural network training acceleration.

The training of neural network requires similar operations as inference. Previous work on inference accelerator design gives hint on training acceleration. Model pruning and quantization have proved to be effective to reduce the requirements of computation, bandwidth and memory footprint, and have almost no effect on the performance (accuracy metric) of neural network inference \cite{han2015deep}. A series of the accelerator designs~\cite{qiu2016going, han2017ese} follow these ideas with customized architecture. Compared with inference, applying pruning and quantization to training are facing more challenges.

The first challenge lies in the optimization of training. Existing work \cite{han2015deep} applies pruning and quantization to training but only in the fine-tune phase after the model converges. The benefit of accelerating the fine-tune phase is quite limited. Using quantization and pruning in early stage of training is risky. Different from inference, training neural networks needs small steps of gradient descent to optimize the model parameters. Using low precision data can lead to convergence failure or accuracy decline. Pruning on the network before convergence can also limit the parameter space of the model and hurt the final training accuracy. 

The second challenge lies in hardware design for processing sparse network training. Existing accelerator designs implements sparse matrix-vector multiplication kernels to skip the zero multipliers in sparse models. We refer to this kind of sparsity as operator-sparse pattern. In the back-propagation phase of training, utilizing sparsity means to avoid computing gradients for those already pruned parameters, which introduce the result-sparse pattern. Error propagation also requires the transposition of a sparse parameter matrix, bringing difficulty in accessing data efficiently from external memory. No existing designs support result-sparse pattern and sparse matrix transposition.

In this work, we address the above two challenges with the following contributions:

\begin{itemize}
  \item We propose a hardware friendly training process with advanced quantization and pruning. Specially, we explore the effect of when to apply quantization and pruning with real tasks.
  \item We design dedicated processing elements (PEs) on FPGA to support both operator-sparse and result-sparse patterns. The sparse matrix transposition function is supported by specific scheduling method with a novel data organization in external memory.
  \item We evaluate a prototype system on FPGA to show the effectiveness of the design. Experimental results show that the proposed accelerator achieves 641GOP/s equivalent performance and 3x better energy efficiency compared with GPU.
\end{itemize}

The rest of this paper is organized as follows. Section~\ref{sec:preliminary} introduces the background of training of a CNN. Section~\ref{sec:rw} reviews previous work on software and hardware level CNN optimization. Section~\ref{sec:training} and section~\ref{sec:hw} introduces the proposed training process and hardware platform respectively. Experimental results are shown in section~\ref{sec:experiment}. Section~\ref{sec:conclusion} concludes this paper.
