\section{Experiment}\label{sec:experiment}
In this section, we first evaluate the proposed training process on real dataset. Then we show the hardware experimental results. 

\subsection{Evaluation of Training}
We first evaluate the proposed training method with a software version implemented with Tensorflow. We use the VGG-11 model~\cite{simonyan2014very} on CIFAR-10 dataset for evaluation. The model consists of 8 convolution layers and 3 fully connected layers. We first test pruning the model after different normal training epochs. We train the model with floating point data and saves checkpoints after every 30 epochs. The saved checkpoints are pruned using the method in section~\ref{sec:training:prune}. The results are shown in Figure~\ref{fig:prune_exp}. The ratio of pruned weights does not vary greatly through the process of normal training. In general, most of the layers can be pruned to 40\% or less of the original size. This brings great potential of acceleration if hardware supports sparsity well.

\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\columnwidth]{figures/pruning_exp.pdf}
  \caption{Ratio of pruned weights after different numbers of normal training epochs. }
  \label{fig:prune_exp}
\end{figure}

Then we test training the pruned models with different weight buffer sizes. We force the total training epochs to be 300. The model accuracy curves are shown in Figure~\ref{fig:fixed_train_exp}. When using 24-bit weight buffer, we see that training still works when gradients are quantized. But when to apply pruning and quantization matters. Starting as early as 30 epochs drops the final model accuracy from 90.62\% to 88.94\%. While starting as late as 240 epochs can even lead to worse result: the accuracy fluctuates around 87\%. Choose somewhere in the middle seems to bring good results but more experiments are needed to verify this conclusion. In this experiment, start quantization and pruning after only 60 epochs still brings 90.65\% accuracy at the end of training. When using 16-bit weight buffer, all the training stops after 120 epochs of training because the learning rate is too small while the bit-width is too short.

\begin{figure*}[tb]
  \centering
  \includegraphics[width=2.0\columnwidth]{figures/fixed_train_exp.pdf}
  \caption{Accuracy curve of different training evaluation when quantization and pruning are applied at different stages. The number for each curve denotes the number of normal training epochs and baseline denotes a training without quantization and pruning. }
  \label{fig:fixed_train_exp}
\end{figure*}

\input{tables/resource_util.tex}

\subsection{Effect of Workload Imbalance}\label{sec:exp:imb}

Before evaluating the hardware performance, we first analyze the theoretical performance loss brought by workload imbalance. As introduced in section~\ref{sec:hw_unroll}, unroll parameters are limited by loop dimension variety and sparsity. As we cannot group the PEs for fully connected layers, we only compare on the convolutional layers. The normalized speed on the convolutional layers of each configuration is shown in Table~\ref{tab:hw_util}.

\input{tables/pe_util.tex}

\begin{figure}[tb]
  \centering
  \includegraphics[width=1.0\columnwidth]{figures/util_real.png}
  \caption{Theoretical utilization ratio for each layer over the number of PEs.}
  \label{fig:util_real}
\end{figure}

Here, we see that using 32 PEs for this network will suffer from $7\%$ performance loss compared with the ideal case where no workload imbalance occurs. Group each 4 PEs together brings about $5\%$ performance increase. 


\subsection{Hardware Performance}
A prototype design is implemented on a Xilinx XCKU115 chip with 2 on-board DDR4 SDRAM. In this system, feature map, neuron and their gradients are all of 8-bit and stored in DDR0. Network weights, each of which includes 8-bit weight for computation and extra 16-bit buffer, are stored in DDR1. Corresponding index for sparse representation are stored in DDR0 using \{y[3:0], x[3:0]\} format. This means the the maximum weights block size can be $16\times 16$. The system operates at 200MHz. Each $2\times 2$ PEs are grouped together for CONV layers. We simulate the performance of each layer in each training step with the DDR model and controller from Xilinx IP. Detailed running time and performance are shown in Table~\ref{tab:layerperformance}.

\input{tables/breakdown.tex}

\input{tables/perf_compare.tex}

The peak performance of the hardware is $250MHz \times 1024DSP \times 2 = 500GOP/s$. For FF and NG steps, the proposed hardware achieves 900GOP/s overall performance which achieves at least $1.8\times$ speedup over a dense accelerator with the same peak performance. From the bound type column, we see that most of the CONV layers with heavy workload are computation bounded. This shows that the proposed accelerator can handle large network well.

Besides that, those layers which are bandwidth bounded gives insights to hardware design methods. The first layer suffers from the bandwidth problem because the channel number for this layer is small. We only cut $3\times 8$ block for this layer. A small block size increases the ratio between the necessary feature maps and the necessary convolution kernels. Compared with the workload imbalance result in \ref{sec:exp:imb}, we see that small layers suffer more on bandwidth rather than workload imbalance. Besides reducing the number PEs, increase the buffer size in PEs can help further explore the data locality of 2-d convolution and improves performance. 

The last few layers also suffers greatly from a limited bandwidth. On the one hand, FC layers and convolution layers with small feature maps are of high bandwidth cost for network parameters. On the other hand, split the parameters into small blocks decreases the memory access efficiency. When upgrading the weights of each layer, the weight buffer consumes more bandwidth and causes the performance loss. Reduce the weight buffer size should also be a future research topic. 

A performance and energy efficiency comparison is with state-of-the-art neural network inference/training accelerators and GPU training result is shown in Table~\ref{tab:perf_compare}. The GPU used for comparison is GTX Titan X GM200. Although GPU achieves about 2x speed compared with FPGA, the FPGA part only consumes 1/5 power compared with GPU and achieves 3x energy efficiency. The proposed accelerator achieves comparable performance and energy efficiency on inference compared with simple inference accelerators









